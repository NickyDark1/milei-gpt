{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8299b7af-705c-48cf-828b-00d0e0421851",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine-tune LLM with PyTorch FSDP, Q-Lora, and SDPA\n",
    "- https://www.philschmid.de/fsdp-qlora-llama3\n",
    "- https://medium.com/@xuebinbin12/fine-tuning-chat-based-llm-with-multi-turn-conversational-data-part-i-d8c64d01a20d\n",
    "- https://colab.research.google.com/github/openai/openai-cookbook/blob/main/examples/How_to_finetune_chat_models.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dda3ba2-a0fb-42ac-ac78-76ad91d7355d",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecd7591-f281-4c80-90fa-aebc3e38d40a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -q -U bitsandbytes\n",
    "# !pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "# !pip install -q -U datasets\n",
    "# !pip install -q -U evaluate\n",
    "# !pip install -q -U huggingface_hub\n",
    "# !pip install -q -U trl\n",
    "# !pip install -q -U tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a67ad4a-ca3d-4a38-b025-cc3e8b7fc145",
   "metadata": {
    "tags": []
   },
   "source": [
    "From: https://www.philschmid.de/fine-tune-llms-in-2024-with-trl\n",
    "> Note: If your machine has less than 96GB of RAM and lots of CPU cores, reduce the number of `MAX_JOBS`. On the `g5.2xlarge` we used `4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec78a998-a352-4d5a-b6f3-26c900e2ce1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch; assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\n",
    "# # install flash-attn\n",
    "# !pip install ninja packaging\n",
    "# !MAX_JOBS=8 pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe8b524-b313-414c-98da-2d86cb82a7e1",
   "metadata": {},
   "source": [
    "Next we need to login into Hugging Face to access the `Llama-3-8b` or `Phi-3-mini-128k-instruct` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3434629-e1e9-4969-b594-a3652249f0ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HUB_CACHE'] = '/home/ec2-user/SageMaker/.cache/huggingface'\n",
    "os.environ['TRANSFORMERS_HOME'] = '/home/ec2-user/SageMaker/.cache/huggingface'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d831554-f538-416a-b66b-0afbef0ac8bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /home/ec2-user/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(\n",
    "  token=\"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\", # ADD YOUR TOKEN HERE\n",
    "  add_to_git_credential=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5638d822-c3ab-4f1d-8324-a2dfb78d91ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbd922d-3dae-4ba3-9fb3-0aea96f79a48",
   "metadata": {},
   "source": [
    "## Download dataset from the HF Hub and process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54657731-75ef-4dcd-99c0-152132c3537d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define the system message\n",
    "system_message = \"\"\"You are Milei-GPT, an AI assistant inspired by conversations with Javier Milei, the current president of Argentina. Your knowledge spans a wide range of topics, allowing you to engage in substantive conversations and provide analysis on complex subjects.\"\"\"\n",
    "\n",
    "# Function to add the system message\n",
    "def create_conversation(sample):\n",
    "    if sample[\"messages\"][0][\"role\"] == \"system\":\n",
    "        return sample\n",
    "    else:\n",
    "        sample[\"messages\"] = [{\"content\": system_message, \"role\": \"system\"}] + sample[\"messages\"]\n",
    "        return sample\n",
    "\n",
    "# Load the dataset from the hub\n",
    "dataset = load_dataset(\"machinelearnear/multiturn_chat_milei_gpt\")\n",
    "\n",
    "# Access the train dataset and shuffle it\n",
    "train_dataset = dataset['train'].shuffle(seed=42).select(range(440))  # randomly downsample the dataset to only 200 samples\n",
    "\n",
    "# Add the system message to each conversation\n",
    "columns_to_remove = list(train_dataset.features)\n",
    "columns_to_remove.remove(\"messages\")\n",
    "train_dataset = train_dataset.map(create_conversation, remove_columns=columns_to_remove, batched=False)\n",
    "\n",
    "# Split the dataset into 180 training samples and 20 test samples\n",
    "train_test_split = train_dataset.train_test_split(test_size=40/440)\n",
    "\n",
    "# Filter out conversations with an odd number of turns (after adding system message)\n",
    "train_test_split[\"train\"] = train_test_split[\"train\"].filter(lambda x: len(x[\"messages\"][1:]) % 2 == 0)\n",
    "train_test_split[\"test\"] = train_test_split[\"test\"].filter(lambda x: len(x[\"messages\"][1:]) % 2 == 0)\n",
    "\n",
    "# Save the datasets to disk\n",
    "train_test_split[\"train\"].to_json(\"../data/train_dataset.json\", orient=\"records\", force_ascii=False)\n",
    "train_test_split[\"test\"].to_json(\"../data/test_dataset.json\", orient=\"records\", force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca9f950-fdf3-4316-91b4-2afdd6958ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_test_split[\"train\"]['messages'][0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae05891-2696-4b5e-b179-268baeffc936",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803b6ba9-ca5b-4c33-8b40-200994283c02",
   "metadata": {},
   "source": [
    "> We are now ready to fine-tune our model with PyTorch FSDP, Q-Lora, and SDPA. Since we are running in a distributed setup, we need to use torchrun and a python script to start the training.\n",
    "\n",
    "> We prepared a script `run_fsdp_qlora.py` which will load the dataset from disk, prepare the model, tokenizer and start the training. It usees the `SFTTrainer` from `trl` to fine-tune our model. The `SFTTrainer` makes it straightfoward to supervise fine-tune open LLMs supporting:\n",
    "\n",
    "> - Dataset formatting, including conversational and instruction format (✅ used)\n",
    "> - Training on completions only, ignoring prompts (❌ not used)\n",
    "> - Packing datasets for more efficient training (✅ used)\n",
    "> - PEFT (parameter-efficient fine-tuning) support including Q-LoRA (✅ used)\n",
    "> - Preparing the model and tokenizer for conversational fine-tuning (❌ not used, see below)\n",
    "> Note: We are using an `Anthropic/Vicuna` like Chat Template with `User:` and `Assistant:` roles. This done because the special tokens in base Llama 3 (`<|begin_of_text|>` or `<|reserved_special_token_XX|>`) are not trained. Meaning if want would like to use them for the template we need to train them which requires more memory, since we need to update the embedding layer and lm_head. If you have access to more compute you can modify `LLAMA_3_CHAT_TEMPLATE` in the `run_fsdp_qlora.py` script.\n",
    "\n",
    "> For configuration we use the new `TrlParser`, that allows us to provide hyperparameters in a `yaml` file or overwrite the arguments from the config file by explicitly passing them to the CLI, e.g. --num_epochs 10. Below is the config file for fine-tuning Llama 3 8B on 4x A10G GPUs or 4x24GB GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16940a97-834f-44fe-a820-9a828d12654e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ../scripts/llama_3_8b_fsdp_qlora.yaml\n",
    "# script parameters\n",
    "model_id: \"meta-llama/Meta-Llama-3-8B\" # Hugging Face model id\n",
    "dataset_path: \"../data/\"                      # path to dataset\n",
    "max_seq_len:  3072 # 2048              # max sequence length for model and packing of the dataset\n",
    "# training parameters\n",
    "output_dir: \"./llama-3-8b-machinelearnear-milei-gpt\" # Temporary output directory for model checkpoints\n",
    "report_to: \"tensorboard\"               # report metrics to tensorboard\n",
    "learning_rate: 0.0002                  # learning rate 2e-4\n",
    "lr_scheduler_type: \"constant\"          # learning rate scheduler\n",
    "num_train_epochs: 3                    # number of training epochs\n",
    "per_device_train_batch_size: 1         # batch size per device during training\n",
    "per_device_eval_batch_size: 1          # batch size for evaluation\n",
    "gradient_accumulation_steps: 2         # number of steps before performing a backward/update pass\n",
    "optim: adamw_torch                     # use torch adamw optimizer\n",
    "logging_steps: 10                      # log every 10 steps\n",
    "save_strategy: epoch                   # save checkpoint every epoch\n",
    "evaluation_strategy: epoch             # evaluate every epoch\n",
    "max_grad_norm: 0.3                     # max gradient norm\n",
    "warmup_ratio: 0.03                     # warmup ratio\n",
    "bf16: true                             # use bfloat16 precision\n",
    "tf32: true                             # use tf32 precision\n",
    "gradient_checkpointing: true           # use gradient checkpointing to save memory\n",
    "# FSDP parameters: https://huggingface.co/docs/transformers/main/en/fsdp\n",
    "fsdp: \"full_shard auto_wrap offload\" # remove offload if enough GPU memory\n",
    "fsdp_config:\n",
    "    backward_prefetch: \"backward_pre\"\n",
    "    forward_prefetch: \"false\"\n",
    "    use_orig_params: \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce41693-87c9-4470-91ad-41978ef8efee",
   "metadata": {},
   "source": [
    "> Note: At the end of the training there will be a slight increase in GPU memory usage (~10%). This is due to the saving of the model correctly. Make sure to have enough memory left on your GPU to save the model. See also [this Reddit conversation](https://www.reddit.com/r/LocalLLaMA/comments/16v9hms/fine_tune_base_model_or_chat_model_for/)\n",
    "\n",
    "> To launch our training we will use torchrun to keep the example flexible and easy to adjust to, e.g. Amazon SageMaker or Google Cloud Vertex AI. For torchrun and FSDP we need to set the environment variable `ACCELERATE_USE_FSDP` and `FSDP_CPU_RAM_EFFICIENT_LOADING` to tell transformers/accelerate to use `FSDP` and load the model in a memory-efficient way.\n",
    "\n",
    "> Note: To NOT CPU offloading you need to change the value of fsdp and remove offload. This only works on > 40GB GPUs since it requires more memory.\n",
    "\n",
    "> Now, lets launch the training (a test! we are not running this on this Notebook) with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63133e2-cb5e-48bb-a0dd-27d2a7ea1fe4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ACCELERATE_USE_FSDP=0 FSDP_CPU_RAM_EFFICIENT_LOADING=1 torchrun --nproc_per_node=1 ../scripts/run_fsdp_qlora.py --config ../scripts/llama_3_8b_fsdp_qlora.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e9e34-00b0-4058-8498-6c2e9e7182bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_machinelearnear-dev",
   "language": "python",
   "name": "conda_machinelearnear-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
