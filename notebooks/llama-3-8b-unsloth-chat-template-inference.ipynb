{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9ebf048-4271-4af1-aad7-61d44dddd0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = 'your-cache-dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "894e2878-93b9-42eb-8125-6552a8b3f4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0908e948a074baebbc6d999c1215d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/741 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth: Fast Llama patching release 2024.5\n",
      "   \\\\   /|    GPU: NVIDIA A10G. Max memory: 22.191 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. Xformers = 0.0.26.post1. FA = True.\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0e6c84489647d587c60113ca076bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.5 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"machinelearnear/llama-3-8b-milei-gpt\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50313b31-3e04-4d06-b0f5-d3164db09119",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Che pero el estado tiene que invertir en trenes o no?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Bueno, eso es una discusi√≥n pol√≠tica.  Yo creo que hay un problema de priorizaci√≥n y de recursos.  Y si vos te fij√°s... El Estado invierte mucho m√°s en carretera que en ferrocarril.  Es decir, la inversi√≥n del Estado en carreteras supera a las inversiones en ferrocarriles por tres veces.  Entonces, ¬øpor qu√© se hace esto as√≠?  Porque los pol√≠ticos est√°n muy ligados al sector privado de la construcci√≥n.  O sea, son amigos, tienen negocios juntos, entonces le dan much√≠simo m√°s importancia a lo que genera empleo directamente en la construcci√≥n.  Pero bueno, digamos, yo estoy con otra visi√≥n.  A ver, cuando vos ten√©s un pa√≠s grande como Argentina, donde est√°bamos cerca de cien millones de habitantes, necesitabas tener un sistema de transporte p√∫blico eficiente para poder movilizarte.  Ese era uno de los grandes problemas argentinos.  Ahora, ese sistema de transporte p√∫blico estaba basado fundamentalmente sobre el ferrocarril.  Hoy tenemos treinta mil kil√≥metros de v√≠as.  De esos treinta mil kil√≥metros, diez mil kil√≥metros funcionan todav√≠a hoy d√≠a.  Los otros veinte mil kil√≥metros est√°n abandonadas.  En algunos casos, porque fue expropiada la empresa, quedaron sin mantenimiento y despu√©s se rompieron.  Otros casos, porque fueron vendidos a empresas particulares y ahora esas empresas no pueden mantenerlos.  Hay muchos casos tambi√©n donde simplemente se dejaron de lado porque nadie quer√≠a pagarle el precio que ped√≠an los due√±os originales.  Bueno, esa situaci√≥n generaba un desastre.  La gente ten√≠a que ir caminando, hab√≠a que hacer cosas raras para llegar a su trabajo.  Era un caos.  Entonces, ¬øqu√© hicimos nosotros?  Nosotros decid√≠amos ponerlo en manos del sector privado.  Lo primero que hizo el gobierno de Menem fue sacar todas estas l√≠neas que estaban abandonadas y darlas a empresas privadas.  Consecuentemente, algunas empresas pudieron recuperarse y otras no.  Despu√©s, cuando llegu√© yo a la presidencia, me encontr√© con este problema enorme.  Entonces, nos pusimos a estudiar c√≥mo podiamos arreglarlo.  Uno de los primeros actos que hice fue llamar a todos estos empresarios que hab√≠an comprado las l√≠neas y les dije, mir√°, acabo de asumir, tengo que resolver esta cosa.  Si ustedes quieren seguir operando, van a tener que cumplir ciertas condiciones.  Primero, van a tener que mejorar la seguridad.  Segundo, van a tener que bajar los precios.  Tercer punto, van a tener que aumentar la cantidad de pasajeros.  Cuarto punto, van a tener que modernizar sus trenes.  Quinto punto, van a tener que empezar a autofinanciarse.  No va a haber ning√∫n subsidio ni nada por el estilo.  Digo, vamos a tratar de hacerlo todo de manera competitiva.  Algunos aceptaron, otros no quisieron aceptarlo y tuvieron que dejar de ser propietario de la l√≠nea.  As√≠ es como fuimos avanzando.  Cuando termin√°bamos mi segundo mandato, ya casi toda la red funcionalizada estaba siendo manejada por el sector privado.  Esto implic√≥ que hubiera mejoramiento en cuanto a la frecuencia, en cuanto a la calidad, en cuanto a la seguridad, en cuanto a los precios.  Fue un gran √©xito.  Lamentablemente, desde ah√≠ hasta adelante, cada vez que ha pasado algo importante, alguien decidi√≥ tirar abajo el proyecto.  Por ejemplo, cuando Cristina Fern√°ndez tom√≥ el poder, ella dijo, no quiero que haya competencia, voy a nacionalizarlo.  Se cre√≥ Cargas Ferroviarias, que b√°sicamente era un organismo que iba a manejar todo el sistema.  B√°sicamente, lo √∫nico que hac√≠a era subir los costos y bajaba la productividad.  Entonces, naturalmente, la gente perdi√≥ confianza.  Adem√°s, adem√°s, la gesti√≥n de Cristina Fern√°ndez fue absolutamente lamentable.  Ella contrat√≥ a un mont√≥n de personas, pag√°ndoles salarios enormes, y nunca trabajaron.  Entonces, claro, eso signific√≥ que la gente perdiera la fe en el sistema.  Y despu√©s vin\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"content\": \"Che pero el estado tiene que invertir en trenes o no?\",\n",
    "        \"role\": \"user\"\n",
    "    },\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(\n",
    "    input_ids = inputs,\n",
    "    streamer = text_streamer,\n",
    "    max_new_tokens = 1000,\n",
    "    use_cache = True,\n",
    "    temperature=0.4, # creativity and randomness of the response\n",
    "    top_p=0.9, # dynamically adjusts the number of choices for each predicted token, which helps to maintain diversity and generate more fluent and natural-sounding text\n",
    "    top_k=30, # limits the number of choices for the next predicted word or token, which helps to speed up the generation process and can improve the quality of the generated text\n",
    "    repetition_penalty=1.2, # reduce the likelihood of repeating prompt text or getting stuck in a loop\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0fef8d-50aa-481f-8498-63ad26357e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a1b363-7a71-4975-b654-e5d1a60115d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_machinelearnear-dev",
   "language": "python",
   "name": "conda_machinelearnear-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
